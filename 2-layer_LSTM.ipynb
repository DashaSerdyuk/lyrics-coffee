{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Packages for scrapping, cleaning and preprocessing data\n",
    "import os\n",
    "import re\n",
    "import random as rd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Packages for LSTM building\n",
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import LambdaCallback\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions for scrapping \n",
    "### Works only for amalgama-lab ~ structure type: site/first_letter_of_artist_name/artist_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup_by_artist(site, artist):\n",
    "    base = site + artist[0] + '/' + artist #get address in needed structure\n",
    "    site = requests.get(base) #get from base\n",
    "    return BeautifulSoup(site.text, 'lxml'), base #return FULL HTML code of got from base\n",
    "\n",
    "def get_raw_lyrics(soup):\n",
    "    songs_list = soup.find(\"div\", {\"id\":\"songs_nav\"})\n",
    "    s = str(list(songs_list)[9])\n",
    "    found_songs = re.findall(r'(?<=<a href=\")[^\"]*', s)\n",
    "    return found_songs\n",
    "\n",
    "def parse_raw_lyrics(retrsongs, base):\n",
    "    songs = {}\n",
    "    for songname in retrsongs:\n",
    "        songraw = requests.get(base + '/' + songname)\n",
    "        parrallel_songs = BeautifulSoup(songraw.text, 'lxml').find(\"div\", {\"id\": \"click_area\"})\n",
    "        if parrallel_songs:\n",
    "            lines = []\n",
    "            for p in parrallel_songs.findAll(\"div\", {\"class\": \"original\"}):\n",
    "                lines.append(str(p.get_text()).replace('\\n',''))\n",
    "            songs[songname.replace('.html', '')] = lines\n",
    "    return songs\n",
    "\n",
    "def write_lyrics_to_file(artist, songs):\n",
    "    newpath =  \"lyrics/\" + artist\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    i = 0\n",
    "    for s in songs.keys():\n",
    "        with open(newpath + '//' + artist + '_{0}.txt'.format(s), 'w', encoding='UTF-8') as ffff:\n",
    "            for l in songs[s]:\n",
    "                ffff.write(l + \"\\n\")\n",
    "        ffff.close()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting all songs in separate files and in separate folders for each artist in folder .../lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "site = 'http://www.amalgama-lab.com/songs/'\n",
    "\n",
    "artists = ['a_rocket_to_the_moon', 'abba', 'ac_dc', 'adam_lambert', 'adele', 'aerosmith', 'anti_flag',\n",
    "           'arctic_monkeys', 'bring_me_the_horizon', 'britney_spears', 'bon_jovi', 'bob_dylan',\n",
    "          'backstreet_boys', 'blink_182', 'black_sabbath', 'depeche_mode', 'david_bowie', 'doors', 'evanescence',\n",
    "          'elvis_presley', 'elton_john', 'elton_john', 'frank_sinatra', 'foo_fighters', 'green_day', 'gorillaz',\n",
    "          'hurts', 'justin_bieber', 'justin_timberlake', 'korn', 'kasabian', 'kiss', 'linkin_park', 'lana_del_rey',\n",
    "          'limp_bizkit', 'metallica', 'maroon_5', 'michael_jackson', 'marilyn_manson', 'nirvana', 'nickelback',\n",
    "          'nightwish', 'onerepublic', 'placebo', 'papa_roach', 'red_hot_chili_peppers', 'rasmus', 'scorpions',\n",
    "          'system_of_a_down', 'three_days_grace', 'u2', 'whitney_houston', 'weeknd']\n",
    "\n",
    "for artist in artists:\n",
    "    print(\"now handling: {0}... \".format(artist), end='')\n",
    "    try:\n",
    "        soup, base = get_soup_by_artist(site, artist)\n",
    "        raw_lyrics = get_raw_lyrics(soup)\n",
    "        lyrics_dict = parse_raw_lyrics(raw_lyrics, base)\n",
    "        write_lyrics_to_file(artist, lyrics_dict)\n",
    "        print('done.')\n",
    "    except Exception as e:\n",
    "        print('failed with {0}.'.format(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting one big corpus in 1 file from all song texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/nikolay/Python/NLP lyrics and coffee/lyrics')\n",
    "\n",
    "# Corpus that constists of songs of ALL artists (~70) is too large\n",
    "# I got random sample of 5 artists and got:\n",
    "# ['scorpions', 'anti_flag', 'bob_dylan', 'green_day', 'bruno_mars']\n",
    "# I will attach full corpus before and after cleaning \n",
    "\n",
    "\n",
    "# list_dir = rd.sample(os.listdir(), 5)\n",
    "k = 0\n",
    "with open('/home/nikolay/Python/NLP lyrics and coffee/result_corpus_large', 'w') as outfile:\n",
    "    for folder in list_dir:\n",
    "        os.chdir('.../NLP lyrics and coffee/lyrics')\n",
    "        filenames = os.listdir(folder)\n",
    "        os.chdir(folder)\n",
    "        for fname in filenames:\n",
    "            infile = open(fname, 'r')\n",
    "            outfile.write(infile.read())\n",
    "outfile.close()\n",
    "os.chdir('/home/nikolay/Python/NLP lyrics and coffee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning result corpus from Russian letters, digits and special symbols\n",
    "\n",
    "test_text = open('/home/nikolay/Python/NLP lyrics and coffee/result_corpus_large', 'r', encoding='UTF-8')\n",
    "\n",
    "with open('/home/nikolay/Python/NLP lyrics and coffee/result_corpus_large_cleaned', 'w') as outfile:\n",
    "    for line in test_text:\n",
    "        line = re.sub(r'[а-яА-Я]|\\d|[\\?\\.\\_\\,\\:\\;\\^\\$\\#\\@\\&\\(\\)\\*\\!\\<\\>\\\"\\'\\{\\}\\[\\]\\~\\+\\-\\=\\/ñ|à|é|ó|ß|á|â|ä|ç|è|é|ê|ì|í|î|ô|ö|ù|ú|ü|œ|ё|ґ|ṗ|‒|“|”|„|き|げ|し|ち|て|上|持|放|\\'|\\`|\\…|\\—|\\–|\\‘|\\’\\¦\\«\\°\\´\\»\\×]', '', line)\n",
    "        line = re.sub(r'  ', ' ', line)\n",
    "        outfile.writelines(line)\n",
    "outfile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "\n",
    "filename = \"result_corpus_large_cleaned\"\n",
    "raw_text = open(filename, encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "csv_logger = CSVLogger('log.csv', append=True, separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  650161\n",
      "Total Vocab:  30\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  650061\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "# 2-layers LSTM Neural Network\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "650048/650061 [============================>.] - ETA: 0s - loss: 2.4085Epoch 00001: loss improved from inf to 2.40845, saving model to weights-improvement-bigger.hdf5\n",
      "650061/650061 [==============================] - 3050s 5ms/step - loss: 2.4085\n",
      "Epoch 2/5\n",
      "650048/650061 [============================>.] - ETA: 0s - loss: 2.0585Epoch 00002: loss improved from 2.40845 to 2.05852, saving model to weights-improvement-bigger.hdf5\n",
      "650061/650061 [==============================] - 3030s 5ms/step - loss: 2.0585\n",
      "Epoch 3/5\n",
      "650048/650061 [============================>.] - ETA: 0s - loss: 1.9318Epoch 00003: loss improved from 2.05852 to 1.93175, saving model to weights-improvement-bigger.hdf5\n",
      "650061/650061 [==============================] - 3027s 5ms/step - loss: 1.9317\n",
      "Epoch 4/5\n",
      "650048/650061 [============================>.] - ETA: 0s - loss: 1.8579Epoch 00004: loss improved from 1.93175 to 1.85788, saving model to weights-improvement-bigger.hdf5\n",
      "650061/650061 [==============================] - 3022s 5ms/step - loss: 1.8579\n",
      "Epoch 5/5\n",
      "650048/650061 [============================>.] - ETA: 0s - loss: 1.8106Epoch 00005: loss improved from 1.85788 to 1.81059, saving model to weights-improvement-bigger.hdf5\n",
      "650061/650061 [==============================] - 3019s 5ms/step - loss: 1.8106\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "# filepath=\"weights-improvement-1.hdf5\"\n",
    "filepath=\"weights-improvement-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "\n",
    "import json\n",
    "json_log = open('loss_log.json', mode='wt', buffering=1)\n",
    "json_logging_callback = LambdaCallback(\n",
    "    on_batch_begin=lambda epoch, logs: json_log.write(\n",
    "        json.dumps({'time' : time.ctime(), 'epoch': epoch, 'loss': logs}) + '\\n'),\n",
    "    on_epoch_end=lambda epoch, logs: json_log.write(\n",
    "        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\\n'),\n",
    "    on_train_end=lambda logs: json_log.close()\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger, json_logging_callback]\n",
    "\n",
    "# Number of epochs and batch_size\n",
    "model.fit(X, y, batch_size=64, callbacks=callbacks_list, epochs=5)\n",
    "\n",
    "print('done');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" ow and bow\n",
      "sara oh sara\n",
      "dont ever leave me dont ever go\n",
      "one by one they followed the sun\n",
      "one by one  \"\n",
      "thing i wanna be the sain\n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind \n",
      "\n",
      "i wanna be the wind\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filepath=\"weights-improvement-bigger.hdf5\"\n",
    "model.load_weights(filepath)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(200):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
